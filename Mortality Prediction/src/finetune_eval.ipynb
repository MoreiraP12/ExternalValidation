{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "def train_model(dataset):\n",
    "    # Load data\n",
    "    icu_data = pd.read_parquet(f'../{dataset}/dyn.parquet')\n",
    "    mortality_data = pd.read_parquet(f'../{dataset}/outc.parquet')\n",
    "    demographic_data = pd.read_parquet(f'../{dataset}/sta.parquet')\n",
    "\n",
    "    # Preprocessing\n",
    "    demographic_data['sex'] = demographic_data['sex'].replace({'Male': 1, 'Female': 0})\n",
    "    mortality_data = mortality_data.merge(demographic_data, on='stay_id')\n",
    "    df = icu_data.merge(mortality_data, on='stay_id')\n",
    "    df.fillna(0, inplace=True)\n",
    "    df['time'] = df['time'].dt.total_seconds()\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=['label'])\n",
    "    y = df['label']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Identify discrete and continuous columns\n",
    "    discrete_cols = ['sex']\n",
    "    continuous_cols = X.columns.difference(discrete_cols)\n",
    "\n",
    "    # Separate discrete and continuous features for training data\n",
    "    X_train_discrete = X_train[discrete_cols]\n",
    "    X_train_continuous = X_train[continuous_cols]\n",
    "\n",
    "    # Apply scaling only to continuous features for training data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_continuous_scaled = scaler.fit_transform(X_train_continuous)\n",
    "\n",
    "    # Combine discrete and scaled continuous features for training data\n",
    "    X_train_scaled = pd.concat([X_train_discrete.reset_index(drop=True), pd.DataFrame(X_train_continuous_scaled, columns=continuous_cols)], axis=1)\n",
    "\n",
    "    # Apply SMOTE to the training data\n",
    "    smote = SMOTE()\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_resampled), y=y_train_resampled)\n",
    "    class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "    # Calculate initial bias for the output layer\n",
    "    initial_bias = np.log([y_train_resampled.sum() / (len(y_train_resampled) - y_train_resampled.sum())])\n",
    "\n",
    "    # Define the neural network architecture\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(X_train_resampled.shape[1],)),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid', bias_initializer=tf.keras.initializers.Constant(initial_bias))\n",
    "    ])\n",
    "\n",
    "    # Compile the model with Binary Crossentropy and from_logits=True\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "    # Early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', mode='max', patience=10, restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train_resampled, y_train_resampled, epochs=1, batch_size=32, validation_split=0.2, class_weight=class_weights, callbacks=[early_stopping])\n",
    "\n",
    "    # Return the trained model, scaler, and column information\n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(dataset, model, scaler, stay_ids=None):\n",
    "    # Load data\n",
    "    icu_data = pd.read_parquet(f'../{dataset}/dyn.parquet')\n",
    "    mortality_data = pd.read_parquet(f'../{dataset}/outc.parquet')\n",
    "    demographic_data = pd.read_parquet(f'../{dataset}/sta.parquet')\n",
    "\n",
    "    # Filter the data if stay_ids is provided\n",
    "    if stay_ids is not None:\n",
    "        icu_data = icu_data[icu_data['stay_id'].isin(stay_ids)]\n",
    "        mortality_data = mortality_data[mortality_data['stay_id'].isin(stay_ids)]\n",
    "        demographic_data = demographic_data[demographic_data['stay_id'].isin(stay_ids)]\n",
    "\n",
    "    # Preprocessing\n",
    "    demographic_data['sex'] = demographic_data['sex'].replace({'Male': 1, 'Female': 0})\n",
    "    mortality_data = mortality_data.merge(demographic_data, on='stay_id')\n",
    "    df = icu_data.merge(mortality_data, on='stay_id')\n",
    "    df.fillna(0, inplace=True)\n",
    "    df['time'] = df['time'].dt.total_seconds()\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=['label'])\n",
    "    y = df['label']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    _, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Identify discrete and continuous columns\n",
    "    discrete_cols = ['sex']\n",
    "    continuous_cols = X.columns.difference(discrete_cols)\n",
    "    \n",
    "    # Apply scaling only to continuous features for test data\n",
    "    X_test_discrete = X_test[discrete_cols]\n",
    "    X_test_continuous = X_test[continuous_cols]\n",
    "    X_test_continuous_scaled = scaler.transform(X_test_continuous)\n",
    "\n",
    "    # Combine discrete and scaled continuous features for test data\n",
    "    X_test_scaled = pd.concat([X_test_discrete.reset_index(drop=True), pd.DataFrame(X_test_continuous_scaled, columns=continuous_cols)], axis=1)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_auc = model.evaluate(X_test_scaled, y_test)\n",
    "    print(f'Test AUC: {test_auc}')\n",
    "\n",
    "    # Predict probabilities for the test set\n",
    "    y_pred_proba = model.predict(X_test_scaled)\n",
    "\n",
    "    # Add gender information to the test set\n",
    "    y_test_df = pd.DataFrame({'label': y_test, 'pred_proba': y_pred_proba.flatten(), 'sex': X_test['sex'], 'stay_id': X_test['stay_id']})\n",
    "\n",
    "    return y_pred_proba, y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_model(new_dataset, model, scaler):\n",
    "    # Load data\n",
    "    icu_data = pd.read_parquet(f'../{new_dataset}/dyn.parquet')\n",
    "    mortality_data = pd.read_parquet(f'../{new_dataset}/outc.parquet')\n",
    "    demographic_data = pd.read_parquet(f'../{new_dataset}/sta.parquet')\n",
    "\n",
    "    # Preprocessing\n",
    "    demographic_data['sex'] = demographic_data['sex'].replace({'Male': 1, 'Female': 0})\n",
    "    mortality_data = mortality_data.merge(demographic_data, on='stay_id')\n",
    "    df = icu_data.merge(mortality_data, on='stay_id')\n",
    "    df.fillna(0, inplace=True)\n",
    "    df['time'] = df['time'].dt.total_seconds()\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=['label'])\n",
    "    y = df['label']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Identify discrete and continuous columns\n",
    "    discrete_cols = ['sex']\n",
    "    continuous_cols = X.columns.difference(discrete_cols)\n",
    "\n",
    "    # Separate discrete and continuous features for training data\n",
    "    X_train_discrete = X_train[discrete_cols]\n",
    "    X_train_continuous = X_train[continuous_cols]\n",
    "\n",
    "    # Apply scaling only to continuous features for training data\n",
    "    X_train_continuous_scaled = scaler.transform(X_train_continuous)\n",
    "\n",
    "    # Combine discrete and scaled continuous features for training data\n",
    "    X_train_scaled = pd.concat([X_train_discrete.reset_index(drop=True), pd.DataFrame(X_train_continuous_scaled, columns=continuous_cols)], axis=1)\n",
    "\n",
    "    # Apply SMOTE to the training data\n",
    "    smote = SMOTE()\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "    # Calculate class weights\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_resampled), y=y_train_resampled)\n",
    "    class_weights = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "    # Fine-tune the model\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', mode='max', patience=10, restore_best_weights=True)\n",
    "    model.fit(X_train_resampled, y_train_resampled, epochs=1, batch_size=32, validation_split=0.2, class_weight=class_weights, callbacks=[early_stopping])\n",
    "\n",
    "    return model, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "\n",
    "def visualize_results(y_test, y_pred_proba):\n",
    "    # Calculate ROC AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_age(demographic_data, min_age, max_age):\n",
    "    filtered_data = demographic_data[(demographic_data[\"age\"] < max_age) & (demographic_data[\"age\"] > min_age)]\n",
    "    return filtered_data[\"stay_id\"].tolist()\n",
    "\n",
    "def filter_by_region(region, patients_df, hospital_df):\n",
    "    merged_df = pd.merge(patients_df, hospital_df, on='hospitalid', how='inner')\n",
    "    region_filtered_df = merged_df[merged_df['region'] == region]\n",
    "    stay_ids = region_filtered_df['patientunitstayid']\n",
    "    \n",
    "    return stay_ids\n",
    "\n",
    "def filter_by_gender(demographic_data, gender):\n",
    "    filtered_data = demographic_data[demographic_data[\"sex\"] == gender]\n",
    "    return filtered_data[\"stay_id\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task \n",
    "datasets = ['mimic','eicu']\n",
    "finetune_datasets = ['mimic','eicu']\n",
    "evaluation_datasets = ['mimic','eicu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_and_evaluate(datasets, evaluation_datasets):\n",
    "    results = {}\n",
    "    male_results = {}\n",
    "    female_results = {}\n",
    "\n",
    "    for dataset in datasets:\n",
    "        model, scaler = train_model(dataset)\n",
    "        \n",
    "        for evaluation_dataset in evaluation_datasets:\n",
    "            evaluate_and_store_results(dataset, evaluation_dataset, model, scaler, results)\n",
    "            evaluate_and_store_gender_results(dataset, evaluation_dataset, model, scaler, male_results, 'Male')\n",
    "            evaluate_and_store_gender_results(dataset, evaluation_dataset, model, scaler, female_results, 'Female')\n",
    "\n",
    "    return results, male_results, female_results\n",
    "\n",
    "def evaluate_and_store_results(dataset, evaluation_dataset, model, scaler, results):\n",
    "    y_pred_proba, y_test_df = test_model(evaluation_dataset, model, scaler)\n",
    "    accuracy = visualize_results(y_test_df['label'], y_pred_proba)\n",
    "\n",
    "    if dataset not in results:\n",
    "        results[dataset] = {}\n",
    "    results[dataset][evaluation_dataset] = accuracy\n",
    "\n",
    "def evaluate_and_store_gender_results(dataset, evaluation_dataset, model, scaler, gender_results, gender):\n",
    "    demographic_data = pd.read_parquet(f'../../{evaluation_dataset}/sta.parquet')\n",
    "    gender_ids = filter_by_gender(demographic_data, gender)\n",
    "    y_pred_proba, y_test_df = test_model(evaluation_dataset, model, scaler, stay_ids=gender_ids)\n",
    "    accuracy = visualize_results(y_test_df['label'], y_pred_proba)\n",
    "\n",
    "    if dataset not in gender_results:\n",
    "        gender_results[dataset] = {}\n",
    "    gender_results[dataset][evaluation_dataset] = accuracy\n",
    "\n",
    "def create_heatmap(data, ax, title):\n",
    "    sns.heatmap(data, annot=True, cmap=\"Blues\", cbar=True, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Evaluation Dataset')\n",
    "    ax.set_ylabel('Training Dataset')\n",
    "\n",
    "# Assume datasets and evaluation_datasets are predefined lists of dataset names\n",
    "results, male_results, female_results = train_and_evaluate(datasets, evaluation_datasets)\n",
    "\n",
    "# Convert results to DataFrames\n",
    "results_df = pd.DataFrame(results)\n",
    "male_results_df = pd.DataFrame(male_results)\n",
    "female_results_df = pd.DataFrame(female_results)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Create heatmaps side by side\n",
    "create_heatmap(male_results_df, axes[0], 'Model Evaluation Accuracy Heatmap (Male)')\n",
    "create_heatmap(female_results_df, axes[1], 'Model Evaluation Accuracy Heatmap (Female)')\n",
    "\n",
    "# Adjust layout and display the heatmaps\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_and_evaluate(datasets, evaluation_datasets):\n",
    "    results = {}\n",
    "    male_results = {}\n",
    "    female_results = {}\n",
    "    age_0_30_results = {}\n",
    "    age_30_60_results = {}\n",
    "    age_60_100_results = {}\n",
    "\n",
    "    for dataset in datasets:\n",
    "        model, scaler = train_model(dataset)\n",
    "        \n",
    "        for evaluation_dataset in evaluation_datasets:\n",
    "            evaluate_and_store_results(dataset, evaluation_dataset, model, scaler, results)\n",
    "            evaluate_and_store_gender_results(dataset, evaluation_dataset, model, scaler, male_results, 'Male')\n",
    "            evaluate_and_store_gender_results(dataset, evaluation_dataset, model, scaler, female_results, 'Female')\n",
    "            evaluate_and_store_age_results(dataset, evaluation_dataset, model, scaler, age_0_30_results, 0, 30)\n",
    "            evaluate_and_store_age_results(dataset, evaluation_dataset, model, scaler, age_30_60_results, 30, 60)\n",
    "            evaluate_and_store_age_results(dataset, evaluation_dataset, model, scaler, age_60_100_results, 60, 100)\n",
    "\n",
    "    return results, male_results, female_results, age_0_30_results, age_30_60_results, age_60_100_results\n",
    "\n",
    "def evaluate_and_store_results(dataset, evaluation_dataset, model, scaler, results):\n",
    "    y_pred_proba, y_test_df = test_model(evaluation_dataset, model, scaler)\n",
    "    accuracy = visualize_results(y_test_df['label'], y_pred_proba)\n",
    "\n",
    "    if dataset not in results:\n",
    "        results[dataset] = {}\n",
    "    results[dataset][evaluation_dataset] = accuracy\n",
    "\n",
    "def evaluate_and_store_gender_results(dataset, evaluation_dataset, model, scaler, gender_results, gender):\n",
    "    demographic_data = pd.read_parquet(f'../../{evaluation_dataset}/sta.parquet')\n",
    "    gender_ids = filter_by_gender(demographic_data, gender)\n",
    "    y_pred_proba, y_test_df = test_model(evaluation_dataset, model, scaler, stay_ids=gender_ids)\n",
    "    accuracy = visualize_results(y_test_df['label'], y_pred_proba)\n",
    "\n",
    "    if dataset not in gender_results:\n",
    "        gender_results[dataset] = {}\n",
    "    gender_results[dataset][evaluation_dataset] = accuracy\n",
    "\n",
    "def evaluate_and_store_age_results(dataset, evaluation_dataset, model, scaler, age_results, min_age, max_age):\n",
    "    demographic_data = pd.read_parquet(f'../../{evaluation_dataset}/sta.parquet')\n",
    "    age_ids = filter_by_age(demographic_data, min_age, max_age)\n",
    "    y_pred_proba, y_test_df = test_model(evaluation_dataset, model, scaler, stay_ids=age_ids)\n",
    "    accuracy = visualize_results(y_test_df['label'], y_pred_proba)\n",
    "\n",
    "    if dataset not in age_results:\n",
    "        age_results[dataset] = {}\n",
    "    age_results[dataset][evaluation_dataset] = accuracy\n",
    "\n",
    "def create_heatmap(data, ax, title):\n",
    "    sns.heatmap(data, annot=True, cmap=\"Blues\", cbar=True, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Evaluation Dataset')\n",
    "    ax.set_ylabel('Training Dataset')\n",
    "\n",
    "# Assume datasets and evaluation_datasets are predefined lists of dataset names\n",
    "results, male_results, female_results, age_0_30_results, age_30_60_results, age_60_100_results = train_and_evaluate(datasets, evaluation_datasets)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
